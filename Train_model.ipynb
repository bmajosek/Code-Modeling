{"cells":[{"cell_type":"markdown","metadata":{"id":"NQuXlEiiXgBa"},"source":["### 1. Define functions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1714745975414,"user":{"displayName":"Bartosz Maj","userId":"12664590039735639842"},"user_tz":-120},"id":"8ev1KMEkXnDH"},"outputs":[],"source":["import os\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, pipeline\n","import torch\n","from nltk.tokenize import RegexpTokenizer\n","import matplotlib.pyplot as plt\n","import re\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"Z0Iuv32gYfi0"},"source":["function to fetch files with given extension \\\n","I remove multi-line comments because they are mostly unnecessary"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1714746200607,"user":{"displayName":"Bartosz Maj","userId":"12664590039735639842"},"user_tz":-120},"id":"GYQxZGiaXLkg"},"outputs":[],"source":["def extract_kotlin_files(source_dir, extension='.kt'):\n","    code = []\n","    comment_pattern = re.compile(r'/\\*.*?\\*/', re.DOTALL)\n","\n","    for root, dirs, files in os.walk(source_dir):\n","        for file in files:\n","            if file.endswith(extension):\n","                file_path = os.path.join(root, file)\n","                with open(file_path, 'r', encoding='utf-8') as f:\n","                    file_content = f.read()\n","                    file_content = re.sub(comment_pattern, '', file_content)\n","                    code.append(file_content.strip())\n","    return code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_data(data, filename):\n","    with open(filename, 'wb') as f:\n","        pickle.dump(data, f)"]},{"cell_type":"markdown","metadata":{"id":"fqcvoyJJmt8A"},"source":["split data to train/test set"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714745799727,"user":{"displayName":"Bartosz Maj","userId":"12664590039735639842"},"user_tz":-120},"id":"j-7mQTNamxbo"},"outputs":[],"source":["def split_data(dataset):\n","    train_files, test_files = train_test_split(dataset, test_size=0.2, random_state=42)\n","    valid_files, test_files = train_test_split(test_files, test_size=0.5, random_state=42)\n","    return (train_files, test_files, valid_files)"]},{"cell_type":"markdown","metadata":{"id":"rHgbnZu9n1Eu"},"source":["model train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n","        item['labels'] = item['input_ids'].clone()\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings['input_ids'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_loss(results):\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(results['train_loss'], label='Train Loss')\n","    plt.plot(results['validation_loss'], label='Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title('Training and Validation Loss Over Time')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714745799727,"user":{"displayName":"Bartosz Maj","userId":"12664590039735639842"},"user_tz":-120},"id":"sgnIS7O_n36c"},"outputs":[],"source":["def fine_tune_model(train_dataset, valid_dataset, model_name=\"microsoft/phi-1_5\"):\n","    model = AutoModelForCausalLM.from_pretrained(model_name)\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    print(\"Model and tokenizer loaded.\")\n","\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","    \n","    train_encodings = tokenizer(train_dataset, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n","    valid_encodings = tokenizer(valid_dataset, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n","\n","    train_dataset = MyDataset(train_encodings)\n","    valid_dataset = MyDataset(valid_encodings)\n","    print(\"Datasets prepared for training.\")\n","\n","    training_args = TrainingArguments(\n","        output_dir='./results',\n","        evaluation_strategy='steps',\n","        eval_steps=10,\n","        save_strategy='steps',\n","        save_steps=10,\n","        num_train_epochs=5,\n","        per_device_train_batch_size=16,\n","        gradient_accumulation_steps=4,\n","        per_device_eval_batch_size=64,\n","        warmup_steps=500,\n","        weight_decay=0.01,\n","        logging_dir='./logs',\n","        logging_steps=10,\n","        load_best_model_at_end=True,\n","        metric_for_best_model='eval_loss',\n","        greater_is_better=False,\n","        fp16=True\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=valid_dataset\n","    )\n","\n","    trainer.train()\n","\n","    train_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n","    validation_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n","    results = {\n","        'train_loss': train_loss,\n","        'validation_loss': validation_loss\n","    }\n","    plot_loss(results)\n","    \n","    return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_model_and_tokenizer(model, tokenizer, model_path=\"model\", tokenizer_path=\"tokenizer\"):\n","    model.save_pretrained(model_path)\n","    tokenizer.save_pretrained(tokenizer_path)"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Run code"]},{"cell_type":"markdown","metadata":{},"source":["extract kotlin and python files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqwiVKp3XpSK"},"outputs":[],"source":["kotlin_files = extract_kotlin_files(\"/path_to_open_source_project\")\n","print(len(kotlin_files))\n","train_files, test_files, valid_files = split_data(kotlin_files)\n","save_data(test_files, 'kotlin_files.pkl')"]},{"cell_type":"markdown","metadata":{},"source":["Train model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model, tokenizer = fine_tune_model(train_files, valid_files)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_model_and_tokenizer(model, tokenizer)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOGrGhj3eU7QJgZxVVwNEq1","mount_file_id":"1KCXymGSRdOD2Zrr3tBGjviBp5c_JYQnU","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
