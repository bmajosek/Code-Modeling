{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_tokenize(code):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[\\@\\{\\}\\[\\]\\(\\)\\;\\.\\,\\<\\>\\=\\+\\-\\*\\/\\!\\&\\|]+')\n",
    "    return tokenizer.tokenize(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_accuracy(predicted_code, expected_code):\n",
    "    predicted_tokens = language_tokenize(predicted_code)\n",
    "    expected_tokens = language_tokenize(expected_code)\n",
    "    \n",
    "    correct_tokens = sum(1 for x, y in zip(predicted_tokens, expected_tokens) if x == y)\n",
    "    total_tokens = min(len(expected_tokens), len(predicted_tokens))\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts_and_completions(files, split_ratio=0.85):\n",
    "    test_data = []\n",
    "    for file_content in files:\n",
    "        split_point = int(len(file_content) * split_ratio)\n",
    "        prompt = file_content[:split_point]\n",
    "        expected_output = file_content[split_point:]\n",
    "        test_data.append((prompt, expected_output))\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_data):\n",
    "    completion = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "    results = []\n",
    "    accuracies = []\n",
    "\n",
    "    for prompt, expected in test_data:\n",
    "\n",
    "        output = completion(prompt, max_new_tokens=512, max_length=512)\n",
    "        generated_text = output[0]['generated_text']\n",
    "        results.append(generated_text)\n",
    "        acc = compute_token_accuracy(generated_text, expected)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    return results, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(accuracies_kt, accuracies_py):\n",
    "    data = [accuracies_kt, accuracies_py]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.boxplot(data, patch_artist=True, labels=['Kotlin Files', 'Python Files'])\n",
    "    plt.title('Comparison of Model Accuracies on Kotlin vs. Python Files')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path=\"model\", tokenizer_path=\"tokenizer\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kotlin_files = load_data('kotlin_files.pkl')\n",
    "python_files = load_data('python_files.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate prompts and completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_splited = generate_prompts_and_completions(kotlin_files)\n",
    "python_files_splited = generate_prompts_and_completions(python_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_kt, accuracies_kt = evaluate_model(model, tokenizer, test_files_splited)\n",
    "results_py, accuracies_py = evaluate_model(model, tokenizer, python_files_splited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(accuracies_kt, accuracies_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kotlin files mean {np.mean(accuracies_kt)}\")\n",
    "print(f\"Kotlin files median {np.median(accuracies_kt)}\")\n",
    "print(f\"Python files mean {np.mean(accuracies_py)}\")\n",
    "print(f\"Python files median {np.median(accuracies_py)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that kotlin files has better mean and median "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
